{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with OpenAssistant falcon-7b model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bender/miniconda3/envs/torch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)model.bin.index.json: 100%|██████████| 16.9k/16.9k [00:00<00:00, 108MB/s]\n",
      "Downloading (…)l-00001-of-00008.bin: 100%|██████████| 1.92G/1.92G [00:06<00:00, 289MB/s]\n",
      "Downloading (…)l-00002-of-00008.bin: 100%|██████████| 1.99G/1.99G [00:07<00:00, 281MB/s]\n",
      "Downloading (…)l-00003-of-00008.bin: 100%|██████████| 1.91G/1.91G [00:06<00:00, 283MB/s]\n",
      "Downloading (…)l-00004-of-00008.bin: 100%|██████████| 1.91G/1.91G [00:06<00:00, 282MB/s]\n",
      "Downloading (…)l-00005-of-00008.bin: 100%|██████████| 1.99G/1.99G [00:06<00:00, 288MB/s]\n",
      "Downloading (…)l-00006-of-00008.bin: 100%|██████████| 1.91G/1.91G [00:06<00:00, 278MB/s]\n",
      "Downloading (…)l-00007-of-00008.bin: 100%|██████████| 1.91G/1.91G [00:06<00:00, 284MB/s]\n",
      "Downloading (…)l-00008-of-00008.bin: 100%|██████████| 921M/921M [00:03<00:00, 283MB/s]\n",
      "Downloading shards: 100%|██████████| 8/8 [00:52<00:00,  6.50s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.98it/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 116/116 [00:00<00:00, 799kB/s]\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/home/bender/miniconda3/envs/torch2/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: A meme is an idea, behavior, style, or style that is passed from one person or group to another through imitation. It is generally used as a joke or as a way to share an idea with friends and family.\n",
      "\n",
      "The word \"meme\" was first coined in 1976 by British biologist Richard Dawkins, who used it to refer to the spread of cultural information from one mind to another. Dawkins was inspired by the idea of genes as units of cultural information that could be passed from one mind to another in a similar way to biological genes.\n",
      "\n",
      "Since then, the meaning and usage of the word \"meme\" have evolved and expanded to encompass a wide range of cultural phenomena, including images, videos, and text-based jokes. The word \"meme\" has become particularly popular on the internet, where it is often used to describe viral trends and cultural phenomena that have gained widespread attention.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"OpenAssistant/falcon-7b-sft-mix-2000\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "input_text=\"<|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|>\"\n",
    "\n",
    "sequences = pipeline(\n",
    "    input_text,\n",
    "    max_length=500,\n",
    "    do_sample=True,\n",
    "    return_full_text=False,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Sender: Johannes Römpp <mailto:johannes.rompp@raeffelstrasse.ch>\n",
      "Gesendet: Montag, 05. Juni :54\n",
      "Betreff: Email Balckup\n",
      "Vielen Dank für das Feedback, das ist sehr hilfreich. Ich werde mich bei\n",
      "dem Thema genauer mit Ihnen über die Technik auseinandersetzen. \n",
      "Viele Grüße\n",
      "Oliver Hunziker\n"
     ]
    }
   ],
   "source": [
    "input_text=\"\"\"<|prompter|>Analysiere folgende Email-Unterhaltung, getrennt durch <<Start Nachricht>> und <<Ende Nachricht>>, nach folgenden Kriterien:\n",
    "                - Sender\n",
    "                - Gesendet\n",
    "                - Betreff\n",
    "                - Nachricht (nur Text, entferne Signaturen, Adressen, Bilder, Links, Disclaimer und Fussnoten)\n",
    "                - Typ (Frage, Antwort, Information, Aufforderung, Werbung...)\n",
    "\n",
    "                Antwort in einer Liste. Einträge getrennt durch <br>. Beispiel:\n",
    "                \n",
    "                <br>\n",
    "                Typ: \n",
    "                Sender: \n",
    "                Gesendet: \n",
    "                Subject: \n",
    "                Nachricht: \n",
    "                <br>\n",
    "\t\t\t\t\n",
    "<<Start Nachricht>>\n",
    "Ich denke, im Archiv sind die Daten als .msg gespeichert. Export werden sie\n",
    "jedenfalls so.\n",
    "Ich bin heute und morgen ausser Haus, aber werde mich gerne danach damit\n",
    "befassen.\n",
    "Viele Grüsse\n",
    "Oliver Hunziker\n",
    "Von: Johannes Römpp   \n",
    "Gesendet: Montag, 5. Juni :59  \n",
    "An: Oliver Hunziker <oliver.>  \n",
    "Betreff: AW: Email Balckup\n",
    "Ja, das Mailarchiv.\n",
    "Und wie sind die Daten dort gespeichert? Da man sie ja einfach\n",
    "wiederherstellen kann, wahrscheinlich im Outlook Format .msg?\n",
    "* * *\n",
    "Von: Oliver Hunziker  \n",
    "Gesendet: Montag, 05. Juni :56  \n",
    "Bis: Johannes Römpp  \n",
    "Betreff: AW: Email Balckup\n",
    "Grüezi Herr Römpp,\n",
    "Sie meinen das Mailarchiv, nehme ich an.  \n",
    "Dabei handelt es sich um eine Datenbank. Ob man daraus einen Volldump\n",
    "herstellen könnte, kann ich nicht sicher sagen, weil ich es noch nie probiert\n",
    "habe. Ich glaube aber schon, dass das möglich sein sollte.\n",
    "  \n",
    "Viele Grüsse\n",
    "Oliver Hunziker\n",
    "Von: Johannes Römpp\n",
    "< (mailto:)>  \n",
    "Gesendet: Montag, 5. Juni :54  \n",
    "An: Oliver Hunziker\n",
    "< (mailto:oliver.)>  \n",
    "Betreff: Email Balckup\n",
    "Guten Tag Herr Hunziker\n",
    "Eine Frage bezüglich des Email Backups: Wie sind die Daten gespeichert? Als\n",
    ".msg oder als .eml?\n",
    "Ich würde gerne meine Emails nutzen, um ein Modell zu trainieren. Sie über die\n",
    "Wiederherrstellung nutzbar zu machen ist zu aufwändig. Daher die Frage, ob man\n",
    "einen Dump des Postfachs nutzen könnte.\n",
    "Vielen Dank für ein kurzes Feedback\n",
    "Gruss\n",
    "Johannes Römpp\n",
    "T\n",
    " (mailto:)\n",
    " (     RAEFFELSTRASSE\n",
    "25, CH-\n",
    "We Engineer Logistics\n",
    "__Please note: Out of office on Tuesdays.__\n",
    "<<Ende Nachricht>>\n",
    "\t\t\t\t\n",
    "\t\t\t\t<|endoftext|><|assistant|>\"\"\"\n",
    "\n",
    "sequences = pipeline(\n",
    "    input_text,\n",
    "    max_length=2000,\n",
    "    do_sample=True,\n",
    "    return_full_text=False,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
