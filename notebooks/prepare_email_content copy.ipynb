{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to test the processing of the email content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get cleaned content and summary by using OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from pandas import DataFrame, concat, read_csv, read_parquet    \n",
    "import requests\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from azure.data.tables import TableServiceClient, TableEntity\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import os\n",
    "from io import BytesIO\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from numpy import array, array_split, float32, set_printoptions\n",
    "from multiprocessing import  Pool\n",
    "import tiktoken\n",
    "import re\n",
    "from itertools import islice\n",
    "import json\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTLOOK_CONTENT_CONNECTION_STRING = os.environ.get('OUTLOOK_CONTENT_CONNECTION_STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content(row):\n",
    "    content = row['content']\n",
    "    content = content.replace(\"\\r\\n\", \"\\r\")\n",
    "    content = re.sub(r\"\\r+\", \"\\r\", content)\n",
    "    content = re.sub(r\"\\[(.*?)\\]\", \" \", content)\n",
    "    \n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unction to count tokens\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funciton to query chatgpt with content, ask for classification and return response\n",
    "def get_completion(content, prompt_index):\n",
    "    prompt1 = f\"\"\"\n",
    "                Analysiere folgende Email-Unterhaltung, getrennt durch <>, nach folgenden Kriterien:\n",
    "                - Sender\n",
    "                - Gesendet\n",
    "                - Betreff\n",
    "                - Nachricht (nur Text, entferne Signaturen, Adressen, Bilder, Links, Disclaimer und Fussnoten)\n",
    "                - Typ (Frage, Antwort, Information, Aufforderung, Werbung...)\n",
    "\n",
    "                Antwort in einer Liste. Einträge getrennt durch <br>. Beispiel:\n",
    "                \n",
    "                <br>\n",
    "                Typ: Frage\n",
    "                Sender: Max Mustermann\n",
    "                Gesendet: 2021-01-01\n",
    "                Subject: Test\n",
    "                Nachricht: Hallo Welt\n",
    "                <br>\n",
    "                <{content}>\n",
    "                \"\"\"\n",
    "    prompt2 = f\"\"\"Erstelle eine Zusammenfassung der Email-Unterhaltung, inklusive der Personen, die daran beteiligt sind.\n",
    "                Beispiel:\n",
    "                Personen: Max Mustermann, Erika Mustermann\n",
    "                Zusammenfassung: In dieser Email-Unterhaltung geht es um..\n",
    "                <{content}>\n",
    "           \"\"\"\n",
    "    if prompt_index == 1:\n",
    "        prompt = prompt1\n",
    "    elif prompt_index == 2:\n",
    "        prompt = prompt2\n",
    "        \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "            model='gpt-3.5-turbo-16k',\n",
    "            messages=messages,\n",
    "            temperature=0, # this is the degree of randomness of the model's output\n",
    "            max_tokens=8000, # this is the maximum number of tokens that the model will generate\n",
    "            n=1, # this is the number of samples to return\n",
    "        )\n",
    "    return response\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = num_tokens_from_string(\"\"\"Erstelle eine Zusammenfassung der folgenden Email-Unterhaltung in <>, inklusive der Personen, die daran beteiligt sind.\n",
    "                Beispiel:\n",
    "                Personen: Max Mustermann, Erika Mustermann\n",
    "                Zusammenfassung: In dieser Email-Unterhaltung geht es um..\"\"\")\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = get_completion(content,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = r[\"choices\"][0][\"message\"][\"content\"]\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "print(r[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 02_prepare_email.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from io import BytesIO\n",
    "import openai\n",
    "from pandas import DataFrame, to_datetime\n",
    "from azure.data.tables import TableServiceClient\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from datetime import date\n",
    "#from tqdm import tqdm\n",
    "#from numpy import array, array_split, float32, set_printoptions\n",
    "#from multiprocessing import  Pool\n",
    "import tiktoken\n",
    "from timeit import default_timer\n",
    "#from itertools import islice\n",
    "#import json\n",
    "\n",
    "os.environ[\"MODIN_CPUS\"] = \"24\"\n",
    "\n",
    "\n",
    "START_TIME = None\n",
    "OUTLOOK_CONTENT_CONNECTION_STRING = os.environ.get('OUTLOOK_CONTENT_CONNECTION_STRING')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#drop_list_PartitionKey = [\"noreply@emeaemail.teams.microsoft.com\", 'Ambassador@mc.ihg.com', 'microsoft-noreply@microsoft.com']\n",
    "\n",
    "#load data from azure storage table and create data frame\n",
    "\n",
    "def load_data():\n",
    "    # Create the TableServiceClient object which will be used to create a container client\n",
    "    connect_str = OUTLOOK_CONTENT_CONNECTION_STRING\n",
    "    table_service = TableServiceClient.from_connection_string(connect_str)\n",
    "    table_name = \"outlookjohannes\"\n",
    "    table_client = table_service.get_table_client(table_name) \n",
    "    documents = []\n",
    "    for entity in table_client.list_entities():\n",
    "        documents.append(entity)\n",
    "    df =DataFrame(documents)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#clean out content\n",
    "def clean_content(row):\n",
    "    content = row['content']\n",
    "    #content = content.replace(\"\\r\\n\", \"\\r\")\n",
    "    content = content.lstrip('>')\n",
    "    content = re.sub(r'\\*{2,}', '',content)\n",
    "    content = re.sub(r\"\\[(.*?)\\]\", \" \", content)\n",
    "    content = re.sub(r\"[^\\x00-\\x7Füöä]+\", \" \", content)\n",
    "    content = re.sub(r\"_{3,}\", \" \", content)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "#unction to count tokens\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    #set encoding for openai\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    #encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    \n",
    "    #groupeby df by conversation id \n",
    "    #drop all in each group except the one with the youngest received_datetime\n",
    "\n",
    "    # Ensure 'timestamp' column is in datetime format\n",
    "    df['received_datetime'] = to_datetime(df['received_datetime'])\n",
    "\n",
    "    #drop rows with empty content\n",
    "    df = df[df['content'].notna()]\n",
    "\n",
    "    # Group by 'conversation_id' and find the row with the maximum 'timestamp'\n",
    "    idx = df.groupby('conversation_id')['received_datetime'].idxmax()\n",
    "\n",
    "    # Use the indices of the rows with the maximum 'timestamp' to create a new DataFrame\n",
    "    df_latest = df.loc[idx]\n",
    "    df_latest.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #df_latest = df_latest[~df_latest['PartitionKey'].isin(drop_list_PartitionKey)]\n",
    "    df_latest.reset_index(drop=True, inplace=True)\n",
    "    df_latest['content_cleaned'] = df_latest.apply(clean_content, axis=1)\n",
    "    df_latest[\"content_length\"] = df_latest[\"content_cleaned\"].apply(lambda x: len(x))\n",
    "    df_latest[\"content_token_lenght\"] = df_latest[\"content\"].apply(lambda x: num_tokens_from_string(x))\n",
    "\n",
    "\n",
    "\n",
    "    return df_latest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#funciton to query chatgpt with content, ask for classification and return response\n",
    "def get_completion(row):\n",
    "    prompt = f\"\"\"\n",
    "                Analysiere folgende Email-Unterhaltung, getrennt durch <>, nach folgenden Kriterien:\n",
    "                - Sender\n",
    "                - Gesendet\n",
    "                - Betreff\n",
    "                - Nachricht (nur Text, keine Signaturen, Adressen, Bilder, Links, Disclaimer oder Fussnoten)\n",
    "                - Typ (Frage, Antwort, Information, Aufforderung, Werbung...)\n",
    "\n",
    "                Antwort als JSON-Objekte in einer Liste. Liste sortiert nach Datum Gesendet, älteste zuerst. \n",
    "                Beispiel:\n",
    "                [{{\"Sender\": \"Max Mustermann\", \"Gesendet\": \"2021-01-01\", \"Betreff\": \"Test\", \"Nachricht\": \"Hallo Welt\", \"Typ\": \"Frage\"}}]\n",
    "                <{row['content']}>\n",
    "                \"\"\"\n",
    "    try:\n",
    "        if row['content_token_lenght'] < 2000:\n",
    "            model = \"gpt-3.5-turbo\"\n",
    "            max_tokens=3800 - row['content_token_lenght']\n",
    "        else:\n",
    "            model = \"gpt-3.5-turbo-16k\"\n",
    "            max_tokens=15500 - row['content_token_lenght']\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0, # this is the degree of randomness of the model's output\n",
    "            max_tokens=max_tokens, # this is the maximum number of tokens that the model will generate\n",
    "            n=1, # this is the number of samples to return\n",
    "        )\n",
    "        return response\n",
    "    except:\n",
    "        response = {\"choices\": [{\"text\": \"Error\"}]}\n",
    "        return response\n",
    "    \n",
    "\n",
    "#function to upload data to azure blob storage\n",
    "def upload_data(df):\n",
    "    #get today's date\n",
    "    today = date.today().strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        #Save to Azure Blob Storage\n",
    "        # Create the BlobServiceClient object which will be used\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(OUTLOOK_CONTENT_CONNECTION_STRING)\n",
    "\n",
    "        container_name = 'outlookcontent'\n",
    "        \n",
    "        # Create a blob client using the local file name as the name for the blob\n",
    "        file_name = today + \"_test2_outlook_data.parquet\"\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)\n",
    "        \n",
    "        # save dataframe to csv\n",
    "        #csv_file = df.to_csv(index=False)\n",
    "\n",
    "        parquet_file = BytesIO()\n",
    "        df.to_parquet(parquet_file,  engine='pyarrow')\n",
    "        parquet_file.seek(0)  # change the stream position back to the beginning after writing\n",
    "        response = blob_client.upload_blob(data=parquet_file, overwrite=True)\n",
    "\n",
    "        \n",
    "    except:\n",
    "        df.to_parquet(today + \"_outlook_data.parquet\", engine='pyarrow')\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "\n",
    "#os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "#if not DASK_RUNNING:\n",
    "#    from dask.distributed import Client, LocalCluster\n",
    "#    cluster = LocalCluster()  # Launches a scheduler and workers locally\n",
    "#    client = Client(cluster)  # Connect to distributed cluster and override default\n",
    "#    print(f\"Started cluster at {cluster.dashboard_link}\")\n",
    "#    DASK_RUNNING = True\n",
    "\n",
    "\n",
    "#print(modin.config.NPartitions.get())\n",
    "\n",
    "\n",
    "    #START_TIME = default_timer()\n",
    "    df = mpd.DataFrame(load_data())\n",
    "    print(modin.config.NPartitions.get())\n",
    "    #for testing\n",
    "    df = df[:25].copy()\n",
    "    df = clean_data(df)\n",
    "    #df = mpd.DataFrame(df)\n",
    "\n",
    "    df[\"content_processed\"]= df.apply(get_completion, axis=1)\n",
    "    df_normal = df._to_pandas()\n",
    "    upload_data(df_normal)\n",
    "    elapsed_time = default_timer() - START_TIME\n",
    "    completed_at = \"{:5.2f}s\".format(elapsed_time)\n",
    "    print(f\"completed in {completed_at}\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(load_data())\n",
    "#print(modin.config.NPartitions.get())\n",
    "#for testing\n",
    "df = df[:25].copy()\n",
    "df = clean_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quopri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode subject if encoded with quopri by checking the encoding is the beginning of the string\n",
    "def decode_subject(value):\n",
    "    if value.startswith(\"=?\"):\n",
    "        #extract encoding\n",
    "        encoding = value.split(\"?\")[1]\n",
    "        subject = quopri.decodestring(value).decode(encoding)\n",
    "        #subject = value.decode(encoding)\n",
    "        #remove encoding from subject\n",
    "        subject = subject.split(\"?\")[3]\n",
    "    else:\n",
    "        subject = value\n",
    "    return subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subject'] = df['subject'].apply(decode_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_str = df[\"subject\"][1]\n",
    "quopri.decodestring(encoded_str).decode('iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = mpd.DataFrame(df)\n",
    "df[\"content_processed\"]= df.apply(get_completion, axis=1)\n",
    "#df_normal = df._to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
