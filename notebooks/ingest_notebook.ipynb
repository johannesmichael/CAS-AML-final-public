{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to play around with different vectorstores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    JSONLoader,\n",
    "    DataFrameLoader\n",
    ")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "#from constants import CHROMA_SETTINGS\n",
    "\n",
    "from pandas import DataFrame, to_datetime, read_parquet\n",
    "load_dotenv()\n",
    "\n",
    "import openai\n",
    "\n",
    "import requests\n",
    "\n",
    "from azure.data.tables import TableServiceClient, TableEntity\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from io import BytesIO\n",
    "from datetime import date\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Load environment variables\n",
    "persist_directory = os.environ.get('PERSIST_DIRECTORY')\n",
    "source_directory = os.environ.get('SOURCE_DIRECTORY', 'source_documents')\n",
    "embeddings_model_name = os.environ.get('EMBEDDINGS_MODEL_NAME')\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from chromadb.config import Settings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define the folder for storing database\n",
    "PERSIST_DIRECTORY = os.environ.get('PERSIST_DIRECTORY')\n",
    "\n",
    "# Define the Chroma settings\n",
    "CHROMA_SETTINGS = Settings(\n",
    "        chroma_db_impl='duckdb+parquet',\n",
    "        persist_directory=PERSIST_DIRECTORY,\n",
    "        anonymized_telemetry=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTLOOK_CONTENT_CONNECTION_STRING = os.environ.get('OUTLOOK_CONTENT_CONNECTION_STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from azure blob storage\n",
    "def get_data(file_name):\n",
    "    try:\n",
    "        # Create the BlobServiceClient object which will be used\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(OUTLOOK_CONTENT_CONNECTION_STRING)\n",
    "\n",
    "        container_name = 'outlookcontent'\n",
    "        #get today's date\n",
    "        # Create a blob client using the local file name as the name for the blob\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)\n",
    "        \n",
    "        #download blob\n",
    "        blob = blob_client.download_blob()\n",
    "        #convert blob to dataframe\n",
    "        df = read_parquet(BytesIO(blob.readall()))\n",
    "        \n",
    "        #convert blob to dataframe\n",
    "        #df =read_csv(BytesIO(blob.readall()), sep=',', encoding='utf-8')\n",
    "        \n",
    "    except Exception as e:\n",
    "        return e.message, e.args\n",
    "\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1587, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_data('2023-07-04test_modindask_outlook_data.parquet')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"finish_reason\"] = df['content_processed'].apply(lambda x: x[\"choices\"][0][\"finish_reason\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop rows with finish_reason is not Error\n",
    "df = df[df['finish_reason'] == 'stop']\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of dictionaries from json in content_processed column\n",
    "def create_list(value):\n",
    "    try:\n",
    "        string_to_list = ast.literal_eval(value[\"choices\"][0][\"message\"][\"content\"])\n",
    "    except:\n",
    "        string_to_list = []\n",
    "    return string_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column with list of dictionaries\n",
    "df['content_processed_list'] = df['content_processed'].apply(create_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_rows = df[df['content_processed_list'].apply(len) == 0]\n",
    "failed_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with empty content_processed_list\n",
    "df = df[df['content_processed_list'].apply(len) > 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a text from the Nachrichtkeys in the json\n",
    "def create_text(data):\n",
    "    output_string = ''\n",
    "    for d in data:\n",
    "        for key, value in d.items():\n",
    "            output_string += str(key) + ': ' + str(value) + '\\n'                \n",
    "        output_string += '\\n'  \n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['content_processed_list'].apply(create_text)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create txt for each row in dataframe column text\n",
    "def create_txt(row):\n",
    "    try:\n",
    "        file_name = \"/home/bender/GIT/CAS_AML_final/emails_processed/\" + row['PartitionKey'] + '.txt'\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(row['text'])\n",
    "    except Exception as e:\n",
    "        return e.message, e.args\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "16      None\n",
       "17      None\n",
       "        ... \n",
       "1572    None\n",
       "1573    None\n",
       "1574    None\n",
       "1575    None\n",
       "1583    None\n",
       "Length: 492, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(create_txt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create a list of dictionaries from the json in the content_processed column\n",
    "def create_list(value):\n",
    "    try:\n",
    "        string_to_list = ast.literal_eval(value[\"choices\"][0][\"message\"][\"content\"])\n",
    "    except:\n",
    "        string_to_list = []\n",
    "    return string_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom document loaders\n",
    "class MyElmLoader(UnstructuredEmailLoader):\n",
    "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                doc = UnstructuredEmailLoader.load(self)\n",
    "            except ValueError as e:\n",
    "                if 'text/html content not found in email' in str(e):\n",
    "                    # Try plain text\n",
    "                    self.unstructured_kwargs[\"content_source\"]=\"text/plain\"\n",
    "                    doc = UnstructuredEmailLoader.load(self)\n",
    "                else:\n",
    "                    raise\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise type(e)(f\"{self.file_path}: {e}\") from e\n",
    "\n",
    "        return doc\n",
    "\n",
    "\n",
    "# Map file extensions to document loaders and their arguments\n",
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    # \".docx\": (Docx2txtLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".eml\": (MyElmLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PDFMinerLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "    # Add more mappings for other file extensions and loaders as needed\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_document(file_path: str) -> Document:\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADER_MAPPING:\n",
    "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()[0]\n",
    "\n",
    "    raise ValueError(f\"Unsupported file extension '{ext}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads all documents from the source documents directory, ignoring specified files\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    for ext in LOADER_MAPPING:\n",
    "        all_files.extend(\n",
    "            glob.glob(os.path.join(source_dir, f\"**/*{ext}\"), recursive=True)\n",
    "        )\n",
    "    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n",
    "\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        results = []\n",
    "        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n",
    "            for i, doc in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n",
    "                results.append(doc)\n",
    "                pbar.update()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(ignored_files: List[str] = []) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents and split in chunks\n",
    "    \"\"\"\n",
    "    print(f\"Loading documents from {source_directory}\")\n",
    "    documents = load_documents(source_directory, ignored_files)\n",
    "    if not documents:\n",
    "        print(\"No new documents to load\")\n",
    "        exit(0)\n",
    "    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_vectorstore_exist(persist_directory: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if vectorstore exists\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(persist_directory, 'index')):\n",
    "        if os.path.exists(os.path.join(persist_directory, 'chroma-collections.parquet')) and os.path.exists(os.path.join(persist_directory, 'chroma-embeddings.parquet')):\n",
    "            list_index_files = glob.glob(os.path.join(persist_directory, 'index/*.bin'))\n",
    "            list_index_files += glob.glob(os.path.join(persist_directory, 'index/*.pkl'))\n",
    "            # At least 3 documents are needed in a working vectorstore\n",
    "            if len(list_index_files) > 3:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Create embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "\n",
    "    if does_vectorstore_exist(persist_directory):\n",
    "        # Update and store locally vectorstore\n",
    "        print(f\"Appending to existing vectorstore at {persist_directory}\")\n",
    "        db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
    "        collection = db.get()\n",
    "        texts = process_documents([metadata['source'] for metadata in collection['metadatas']])\n",
    "        print(f\"Creating embeddings. May take some minutes...\")\n",
    "        db.add_documents(texts)\n",
    "    else:\n",
    "        # Create and store locally vectorstore\n",
    "        print(\"Creating new vectorstore\")\n",
    "        texts = process_documents()\n",
    "        print(f\"Creating embeddings. May take some minutes...\")\n",
    "        db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=CHROMA_SETTINGS)\n",
    "    db.persist()\n",
    "    db = None\n",
    "\n",
    "    print(f\"Ingestion complete! You can now run privateGPT.py to query your documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "from redis.commands.search.indexDefinition import (\n",
    "    IndexDefinition,\n",
    "    IndexType\n",
    ")\n",
    "from redis.commands.search.query import Query\n",
    "from redis.commands.search.field import (\n",
    "    TextField,\n",
    "    VectorField\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from azure.data.tables import TableServiceClient, TableEntity\n",
    "from pandas import DataFrame, to_datetime, read_parquet\n",
    "from numpy import array, float32\n",
    "import openai\n",
    "from datetime import datetime, date\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTLOOK_CONTENT_CONNECTION_STRING = os.environ.get('OUTLOOK_CONTENT_CONNECTION_STRING')\n",
    "OPENAI_API_KEY= os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTLOOK_CONTENT_CONNECTION_STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from azure blob storage\n",
    "def get_data(file_name):\n",
    "    try:\n",
    "        # Create the BlobServiceClient object which will be used\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(OUTLOOK_CONTENT_CONNECTION_STRING)\n",
    "\n",
    "        container_name = 'outlookcontent'\n",
    "        #get today's date\n",
    "        #today = date.today().strftime('%Y-%m-%d')\n",
    "        # Create a blob client using the local file name as the name for the blob\n",
    "        \n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)\n",
    "        \n",
    "        #download blob\n",
    "        blob = blob_client.download_blob()\n",
    "        #convert blob to dataframe\n",
    "        df = read_parquet(BytesIO(blob.readall()))\n",
    "        \n",
    "    except Exception as e:\n",
    "        return e.message, e.args\n",
    "\n",
    "    else:\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(\"2023-06-23_outlook_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1375, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['content_ada_embedding'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PartitionKey', 'RowKey', 'message_id', 'subject', 'content', 'sender',\n",
       "       'recipient', 'received_datetime', 'conversation_id', 'web_link',\n",
       "       'content2embed', 'content_ada_embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1371, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop rows where content_ada_embedding is empty list\n",
    "data = data[data['content_ada_embedding'].map(len) > 0]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIS_PASSWORD = os.environ.get('REDIS_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to Redis\n",
    "redis_client = redis.Redis(\n",
    "    host=\"redis-17123.c56.east-us.azure.cloud.redislabs.com\",\n",
    "    port=17123,\n",
    "    password=REDIS_PASSWORD\n",
    ")\n",
    "redis_client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "VECTOR_DIM = 1536 # length of the vectors\n",
    "VECTOR_NUMBER = len(data)                 # initial number of vectors\n",
    "INDEX_NAME = \"openai_embeddings-index\"           # name of the search index\n",
    "PREFIX = \"outlook\"                            # prefix for the document keys\n",
    "DISTANCE_METRIC = \"COSINE\"                # distance metric for the vectors (ex. COSINE, IP, L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RediSearch fields for each of the columns in the dataset\n",
    "subject = TextField(name=\"subject\")\n",
    "web_link = TextField(name=\"web_link\")\n",
    "#conversation_id = TextField(name=\"conversation_id\")\n",
    "text = TextField(name=\"content2embed\")\n",
    "\n",
    "content_ada_embedding = VectorField(\"content_ada_embedding\",\n",
    "    \"FLAT\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": VECTOR_DIM,\n",
    "        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n",
    "        \"INITIAL_CAP\": VECTOR_NUMBER,\n",
    "    }\n",
    ")\n",
    "fields = [subject, web_link, text, content_ada_embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index already exists\n"
     ]
    }
   ],
   "source": [
    "# Check if index exists\n",
    "try:\n",
    "    redis_client.ft(INDEX_NAME).info()\n",
    "    print(\"Index already exists\")\n",
    "except:\n",
    "    # Create RediSearch Index\n",
    "    redis_client.ft(INDEX_NAME).create_index(\n",
    "        fields = fields,\n",
    "        definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_documents(client: redis.Redis, prefix: str, documents: DataFrame):\n",
    "    records = documents.to_dict(\"records\")\n",
    "    for doc in records:\n",
    "        key = f\"{prefix}:{str(doc['message_id'])}\"\n",
    "\n",
    "        # create byte vectors for title and content\n",
    "        \n",
    "        content_ada_embedding = array(doc[\"content_ada_embedding\"], dtype=float32).tobytes()\n",
    "\n",
    "        # replace list of floats with byte vectors\n",
    "        \n",
    "        doc[\"content_ada_embedding\"] = content_ada_embedding\n",
    "\n",
    "        client.hset(key, mapping = doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data[['message_id', 'subject', 'web_link', 'content2embed', 'content_ada_embedding']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "OOM command not allowed when used memory > 'maxmemory'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m index_documents(redis_client, PREFIX, data1)\n",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m, in \u001b[0;36mindex_documents\u001b[0;34m(client, prefix, documents)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m# replace list of floats with byte vectors\u001b[39;00m\n\u001b[1;32m     12\u001b[0m doc[\u001b[39m\"\u001b[39m\u001b[39mcontent_ada_embedding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m content_ada_embedding\n\u001b[0;32m---> 14\u001b[0m client\u001b[39m.\u001b[39mhset(key, mapping \u001b[39m=\u001b[39m doc)\n",
      "File \u001b[0;32m~/miniconda3/envs/py11/lib/python3.11/site-packages/redis/commands/core.py:4952\u001b[0m, in \u001b[0;36mHashCommands.hset\u001b[0;34m(self, name, key, value, mapping, items)\u001b[0m\n\u001b[1;32m   4949\u001b[0m     \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m mapping\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   4950\u001b[0m         items\u001b[39m.\u001b[39mextend(pair)\n\u001b[0;32m-> 4952\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecute_command(\u001b[39m\"\u001b[39m\u001b[39mHSET\u001b[39m\u001b[39m\"\u001b[39m, name, \u001b[39m*\u001b[39mitems)\n",
      "File \u001b[0;32m~/miniconda3/envs/py11/lib/python3.11/site-packages/redis/client.py:1269\u001b[0m, in \u001b[0;36mRedis.execute_command\u001b[0;34m(self, *args, **options)\u001b[0m\n\u001b[1;32m   1266\u001b[0m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnection \u001b[39mor\u001b[39;00m pool\u001b[39m.\u001b[39mget_connection(command_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m   1268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1269\u001b[0m     \u001b[39mreturn\u001b[39;00m conn\u001b[39m.\u001b[39mretry\u001b[39m.\u001b[39mcall_with_retry(\n\u001b[1;32m   1270\u001b[0m         \u001b[39mlambda\u001b[39;00m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_command_parse_response(\n\u001b[1;32m   1271\u001b[0m             conn, command_name, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1272\u001b[0m         ),\n\u001b[1;32m   1273\u001b[0m         \u001b[39mlambda\u001b[39;00m error: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disconnect_raise(conn, error),\n\u001b[1;32m   1274\u001b[0m     )\n\u001b[1;32m   1275\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnection:\n",
      "File \u001b[0;32m~/miniconda3/envs/py11/lib/python3.11/site-packages/redis/retry.py:46\u001b[0m, in \u001b[0;36mRetry.call_with_retry\u001b[0;34m(self, do, fail)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m         \u001b[39mreturn\u001b[39;00m do()\n\u001b[1;32m     47\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_supported_errors \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m     48\u001b[0m         failures \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py11/lib/python3.11/site-packages/redis/client.py:1270\u001b[0m, in \u001b[0;36mRedis.execute_command.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1266\u001b[0m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnection \u001b[39mor\u001b[39;00m pool\u001b[39m.\u001b[39mget_connection(command_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m   1268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1269\u001b[0m     \u001b[39mreturn\u001b[39;00m conn\u001b[39m.\u001b[39mretry\u001b[39m.\u001b[39mcall_with_retry(\n\u001b[0;32m-> 1270\u001b[0m         \u001b[39mlambda\u001b[39;00m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_command_parse_response(\n\u001b[1;32m   1271\u001b[0m             conn, command_name, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1272\u001b[0m         ),\n\u001b[1;32m   1273\u001b[0m         \u001b[39mlambda\u001b[39;00m error: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disconnect_raise(conn, error),\n\u001b[1;32m   1274\u001b[0m     )\n\u001b[1;32m   1275\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnection:\n",
      "File \u001b[0;32m~/miniconda3/envs/py11/lib/python3.11/site-packages/redis/client.py:1246\u001b[0m, in \u001b[0;36mRedis._send_command_parse_response\u001b[0;34m(self, conn, command_name, *args, **options)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m \u001b[39mSend a command and parse the response\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m conn\u001b[39m.\u001b[39msend_command(\u001b[39m*\u001b[39margs)\n\u001b[0;32m-> 1246\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_response(conn, command_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/miniconda3/envs/py11/lib/python3.11/site-packages/redis/client.py:1286\u001b[0m, in \u001b[0;36mRedis.parse_response\u001b[0;34m(self, connection, command_name, **options)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         options\u001b[39m.\u001b[39mpop(NEVER_DECODE)\n\u001b[1;32m   1285\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1286\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39mread_response()\n\u001b[1;32m   1287\u001b[0m \u001b[39mexcept\u001b[39;00m ResponseError:\n\u001b[1;32m   1288\u001b[0m     \u001b[39mif\u001b[39;00m EMPTY_RESPONSE \u001b[39min\u001b[39;00m options:\n",
      "File \u001b[0;32m~/miniconda3/envs/py11/lib/python3.11/site-packages/redis/connection.py:897\u001b[0m, in \u001b[0;36mAbstractConnection.read_response\u001b[0;34m(self, disable_decoding, disconnect_on_error)\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_health_check \u001b[39m=\u001b[39m time() \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhealth_check_interval\n\u001b[1;32m    896\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(response, ResponseError):\n\u001b[0;32m--> 897\u001b[0m     \u001b[39mraise\u001b[39;00m response\n\u001b[1;32m    898\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "\u001b[0;31mResponseError\u001b[0m: OOM command not allowed when used memory > 'maxmemory'."
     ]
    }
   ],
   "source": [
    "index_documents(redis_client, PREFIX, data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoaded \u001b[39m\u001b[39m{\u001b[39;00mredis_client\u001b[39m.\u001b[39minfo()[\u001b[39m'\u001b[39m\u001b[39mTest\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mkeys\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m documents in Redis search index with name: \u001b[39m\u001b[39m{\u001b[39;00mINDEX_NAME\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Test'"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {redis_client.info()['Test']['keys']} documents in Redis search index with name: {INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
