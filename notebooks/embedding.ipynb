{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to test embeddings and ingestion to chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from pandas import DataFrame,  read_parquet, read_csv, concat, ExcelWriter\n",
    "import requests\n",
    "##from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "#from langchain.docstore.document import Document\n",
    "#from azure.data.tables import TableServiceClient, TableEntity\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import os\n",
    "from io import BytesIO\n",
    "from datetime import date\n",
    "#from multiprocessing import  Pool\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTLOOK_CONTENT_CONNECTION_STRING = os.environ.get('OUTLOOK_CONTENT_CONNECTION_STRING')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from azure blob storage\n",
    "def get_data(file_name):\n",
    "    try:\n",
    "        # Create the BlobServiceClient object which will be used\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(OUTLOOK_CONTENT_CONNECTION_STRING)\n",
    "\n",
    "        container_name = 'outlookcontent'\n",
    "        #get today's date\n",
    "        # Create a blob client using the local file name as the name for the blob\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)\n",
    "        \n",
    "        #download blob\n",
    "        blob = blob_client.download_blob()\n",
    "        #convert blob to dataframe\n",
    "        df = read_parquet(BytesIO(blob.readall()))\n",
    "        \n",
    "        #convert blob to dataframe\n",
    "        #df =read_csv(BytesIO(blob.readall()), sep=',', encoding='utf-8')\n",
    "        \n",
    "    except Exception as e:\n",
    "        return e.message, e.args\n",
    "\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data('content_processed.parquet')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = get_data('content_processed_1.parquet')\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"finish_reason\"] = df['content_processed'].apply(lambda x: x[\"choices\"][0][\"finish_reason\"])\n",
    "#drop rows with finish_reason is not Error\n",
    "df[df['finish_reason'] == 'Error'].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"finish_reason\"] = df1['content_processed'].apply(lambda x: x[\"choices\"][0][\"finish_reason\"])\n",
    "#drop rows with finish_reason is not Error\n",
    "df1[df1['finish_reason'] == 'Error'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[df1['finish_reason'] != 'Error']\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge dataframes with concat\n",
    "df_final = concat([df, df1], ignore_index=True)\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicates\n",
    "df_final[df_final.duplicated(subset=['PartitionKey'])].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to upload data to azure blob storage\n",
    "def upload_data(df):\n",
    "    #get today's date\n",
    "    today = date.today().strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        #Save to Azure Blob Storage\n",
    "        # Create the BlobServiceClient object which will be used\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(OUTLOOK_CONTENT_CONNECTION_STRING)\n",
    "\n",
    "        container_name = 'outlookcontent'\n",
    "        \n",
    "        # Create a blob client using the local file name as the name for the blob\n",
    "        file_name = today + \"_final_data.parquet\"\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)\n",
    "        \n",
    "        # save dataframe to csv\n",
    "        #csv_file = df.to_csv(index=False)\n",
    "\n",
    "        parquet_file = BytesIO()\n",
    "        df.to_parquet(parquet_file,  engine='pyarrow')\n",
    "        parquet_file.seek(0)  # change the stream position back to the beginning after writing\n",
    "        response = blob_client.upload_blob(data=parquet_file, overwrite=True)\n",
    "\n",
    "        \n",
    "    except:\n",
    "        df.to_parquet(today + \"_outlook_data.parquet\", engine='pyarrow')\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get completion from ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funciton to query chatgpt with content, ask for classification and return response\n",
    "def get_completion(row):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "                Analysiere folgende Email-Unterhaltung, getrennt durch <>, nach folgenden Kriterien:\n",
    "                - Sender\n",
    "                - Gesendet (Datum)\n",
    "                - Betreff\n",
    "                - Nachricht (nur Text, keine Signaturen, Adressen, Bilder, Links, Disclaimer oder Fussnoten)\n",
    "                - Typ (Frage, Antwort, Information, Aufforderung, Werbung...)\n",
    "\n",
    "                Antwort als JSON-Objekte in einer Liste. Liste sortiert nach Datum Gesendet, Ã¤lteste zuerst. \n",
    "                Beispiel:\n",
    "                [{{\"Sender\": \"Max Mustermann\", \"Gesendet\": \"2021-01-01\", \"Betreff\": \"Test\", \"Nachricht\": \"Hallo Welt\", \"Typ\": \"Frage\"}}]\n",
    "                <{row['content']}>\n",
    "                \"\"\"\n",
    "    try:\n",
    "        if row['content_tt_token_lenght'] < 2000:\n",
    "            model = \"gpt-3.5-turbo\"\n",
    "            max_tokens=3800 - row['content_tt_token_lenght']\n",
    "        else:\n",
    "            model = \"gpt-3.5-turbo-16k\"\n",
    "            max_tokens=15500 - row['content_tt_token_lenght']\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0, # this is the degree of randomness of the model's output\n",
    "            max_tokens=max_tokens, # this is the maximum number of tokens that the model will generate\n",
    "            n=1, # this is the number of samples to return\n",
    "        )\n",
    "        return response\n",
    "    except:\n",
    "        response = {\"choices\": [{\"finish_reason\": \"Error\"}]}\n",
    "        return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns finish_reason and content_processed\n",
    "df.drop(columns=['finish_reason', 'content_processed'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content_processed\"] = df.apply(get_completion, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows where finish_reason is length\n",
    "df_final = df_final[df_final['finish_reason'] != 'length']\n",
    "df_final.dropna(subset=['finish_reason'], inplace=True)\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of dictionaries from json in content_processed column\n",
    "def create_list(value):\n",
    "    try:\n",
    "        return ast.literal_eval(value[\"choices\"][0][\"message\"][\"content\"])\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column with list of dictionaries\n",
    "df_final['content_processed_list'] = df_final['content_processed'].apply(create_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for empty lists in content_processed_list column\n",
    "df_final = df_final[df_final['content_processed_list'].apply(len) != 0]\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a text from the Nachrichtkeys in the json\n",
    "def create_text(value):\n",
    "    output_string = ''\n",
    "    for d in value:\n",
    "        for key, value in d.items():\n",
    "            output_string += str(key) + ': ' + str(value) + '\\n'                \n",
    "        output_string += '\\n'  \n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['text'] = df_final['content_processed_list'].apply(create_text)\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape of empty text\n",
    "check = df_final[df_final['text'] == '']\n",
    "check.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert datetime to timezone unaware\n",
    "check['received_datetime'] = check['received_datetime'].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ExcelWriter('empty_text.xlsx') as writer:\n",
    "    check.to_excel(writer, sheet_name='empty_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_parquet('/home/bender/GIT/CAS_AML_final/db/chroma-embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split text with langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader, DataFrameLoader\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['display_text'] = df_final['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load = df_final[['subject', 'content','conversation_id', 'web_link', 'display_text', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(df_load, page_content_column=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "import chromadb\n",
    "load_dotenv()\n",
    "\n",
    "# Define the folder for storing database\n",
    "PERSIST_DIRECTORY = os.environ.get('PERSIST_DIRECTORY')\n",
    "\n",
    "# Define the Chroma settings\n",
    "CHROMA_SETTINGS = Settings(\n",
    "        chroma_db_impl='duckdb+parquet',\n",
    "        persist_directory=PERSIST_DIRECTORY,\n",
    "        anonymized_telemetry=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=OPENAI_API_KEY,\n",
    "                model_name=\"text-embedding-ada-002\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = chromadb.Client(CHROMA_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(name=\"test\", embedding_function=openai_ef)\n",
    "collection = client.get_collection(name=\"test\", embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_collection(texts):\n",
    "    for text in texts[:2]:\n",
    "        collection.add(documents=text.page_content, metadatas=text.metadata, ids=text.metadata['conversation_id'])\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_collection(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "model=\"text-embedding-ada-002\"\n",
    "#embeddings_generator = openai.Embedding.create(input = [text], model=model)\n",
    "embeddings = [openai.Embedding.create(input = doc.page_content, model=model) for doc in texts]\n",
    "\n",
    "# Extract metadata\n",
    "metadata = [doc.metadata for doc in texts]\n",
    "\n",
    "# Create DataFrame\n",
    "df_embeddings = DataFrame(metadata)\n",
    "df_embeddings['embedding'] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid \n",
    "#create id column\n",
    "df_embeddings['uuid'] = [str(uuid.uuid4()) for _ in range(len(df_embeddings.index))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to upload data to azure blob storage\n",
    "def upload_data(df):\n",
    "    \n",
    "    #Save to Azure Blob Storage\n",
    "    # Create the BlobServiceClient object which will be used\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(OUTLOOK_CONTENT_CONNECTION_STRING)\n",
    "    container_name = 'outlookcontent'\n",
    "    #get today's date\n",
    "    today = date.today().strftime('%Y-%m-%d')\n",
    "    # Create a blob client using the local file name as the name for the blob\n",
    "    file_name = today + \"_outlook_ada_embeddings\"\n",
    "    \n",
    "        \n",
    "    \n",
    "    extension = '.parquet'\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name+extension)\n",
    "    parquet_file = BytesIO()\n",
    "    df.to_parquet(parquet_file,  engine='pyarrow')\n",
    "    parquet_file.seek(0)  # change the stream position back to the beginning after writing\n",
    "    return blob_client.upload_blob(data=parquet_file, overwrite=True)\n",
    "        \n",
    "\n",
    "        \n",
    "    #except:\n",
    "    #    extension = '.csv'\n",
    "    #    blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name+extension)\n",
    "    #    csv_file = BytesIO()\n",
    "    #    df.to_csv(csv_file, index=False)\n",
    "    #    csv_file.seek(0)  # change the stream position back to the beginning after writing\n",
    "    #    return blob_client.upload_blob(data=csv_file, overwrite=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(df_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add to collection\n",
    "def add_to_collection(row):\n",
    "    collection.add(documents=row['content'],\n",
    "    embeddings=row['embedding']['data'][0]['embedding'],\n",
    "     metadatas=row[['subject', 'conversation_id', 'web_link']].to_dict(),\n",
    "    ids=[str(row['uuid'])])\n",
    "    return True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeddings.apply(add_to_collection, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(name=\"openai_ada\", embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.query(\n",
    "    query_texts=[\"Simon Galli\", \"Email\"],\n",
    "    n_results=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeddings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_vectorstore_exist(persist_directory: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if vectorstore exists\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(persist_directory, 'index')):\n",
    "        if os.path.exists(os.path.join(persist_directory, 'chroma-collections.parquet')) and os.path.exists(os.path.join(persist_directory, 'chroma-embeddings.parquet')):\n",
    "            list_index_files = glob.glob(os.path.join(persist_directory, 'index/*.bin'))\n",
    "            list_index_files += glob.glob(os.path.join(persist_directory, 'index/*.pkl'))\n",
    "            # At least 3 documents are needed in a working vectorstore\n",
    "            if len(list_index_files) > 3:\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for rows where text contains \"Simor Galli\"\n",
    "\n",
    "df_final[df_final['text'].str.contains(\"Simon Galli\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = get_data(\"\")\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"finish_reason\"] = df_test['content_summary'].apply(lambda x: x[\"choices\"][0][\"finish_reason\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test['finish_reason'] == 'Error'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test['content_summary'] == 'Error'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['embedding'].is_null().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if does_vectorstore_exist(persist_directory):\n",
    "    # Update and store locally vectorstore\n",
    "    print(f\"Appending to existing vectorstore at {persist_directory}\")\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
    "    collection = db.get()\n",
    "    texts = process_documents([metadata['source'] for metadata in collection['metadatas']])\n",
    "    print(f\"Creating embeddings. May take some minutes...\")\n",
    "    db.add_documents(texts)\n",
    "else:\n",
    "    # Create and store locally vectorstore\n",
    "    print(\"Creating new vectorstore\")\n",
    "    texts = process_documents()\n",
    "    print(f\"Creating embeddings. May take some minutes...\")\n",
    "    db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=CHROMA_SETTINGS)\n",
    "db.persist()\n",
    "db = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_embedding(content, model=\"text-embedding-ada-002\"):\n",
    "    text = content\n",
    "    try:\n",
    "        return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "    except:\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['adda_embedding'] = df['text'].apply(get_openai_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func, n_cores=4):\n",
    "    df_split = array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_temp['content_ada_embedding'] = df_temp.content2embed.progress_apply(lambda x: get_openai_embedding(x, model='text-embedding-ada-002'))\n",
    "\n",
    "df_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rows with empty embedding\n",
    "#df_temp[df_temp['content_ada_embedding'].map(len) == 0]\n",
    "#get rows with PartitionKey == noreply@emeaemail.teams.microsoft.com\n",
    "df_temp[df_temp['PartitionKey'] == 'noreply@emeaemail.teams.microsoft.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.iloc[282]['web_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to upload data to azure blob storage\n",
    "def upload_data(df):\n",
    "    try:\n",
    "        #Save to Azure Blob Storage\n",
    "        # Create the BlobServiceClient object which will be used\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(OUTLOOK_CONTENT_CONNECTION_STRING)\n",
    "\n",
    "        container_name = 'outlookcontent'\n",
    "        #get today's date\n",
    "        today = date.today().strftime('%Y-%m-%d')\n",
    "        # Create a blob client using the local file name as the name for the blob\n",
    "        file_name = today + \"_outlook_data.parquet\"\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)\n",
    "        \n",
    "\n",
    "        parquet_file = BytesIO()\n",
    "        df.to_parquet(parquet_file,  engine='pyarrow')\n",
    "        parquet_file.seek(0)  # change the stream position back to the beginning after writing\n",
    "        response = blob_client.upload_blob(data=parquet_file, overwrite=True)\n",
    "\n",
    "        \n",
    "    except:\n",
    "        print(\"error uploading data to blob storage\")\n",
    "    else:\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from azure blob storage\n",
    "def get_data(file_name):\n",
    "    try:\n",
    "        # Create the BlobServiceClient object which will be used\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(OUTLOOK_CONTENT_CONNECTION_STRING)\n",
    "\n",
    "        container_name = 'outlookcontent'\n",
    "        #get today's date\n",
    "        today = date.today().strftime('%Y-%m-%d')\n",
    "        # Create a blob client using the local file name as the name for the blob\n",
    "        \n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)\n",
    "        \n",
    "        #download blob\n",
    "        blob = blob_client.download_blob()\n",
    "        #convert blob to dataframe\n",
    "        df = read_parquet(BytesIO(blob.readall()))\n",
    "        \n",
    "        #convert blob to dataframe\n",
    "        #df =read_csv(BytesIO(blob.readall()), sep=',', encoding='utf-8')\n",
    "        \n",
    "    except Exception as e:\n",
    "        return e.message, e.args\n",
    "\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
